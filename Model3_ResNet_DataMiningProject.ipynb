{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o88_SzAIG4xM",
        "outputId": "86c36829-50cb-43bb-bf55-fec660bb0994"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Apr 19 23:18:37 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   31C    P0              43W / 400W |      2MiB / 40960MiB |      0%      Default |\n",
            "|                                         |                      |             Disabled |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "u4InyCS4HNMn"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nibabel as nib\n",
        "import scipy.ndimage\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.metrics import MeanSquaredError\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv3D, MaxPooling3D, Flatten, Dense, LeakyReLU, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U5Y9EQTTHQ_J"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lYVrtgFRHa09"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D_umJxy6HgnB"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def augment_3d(image, label):\n",
        "    # Randomly flip the image along the axial plane\n",
        "    if np.random.rand() > 0.5:\n",
        "        image = np.flip(image, axis=0)\n",
        "\n",
        "    # Randomly flip the image along the coronal plane\n",
        "    if np.random.rand() > 0.5:\n",
        "        image = np.flip(image, axis=1)\n",
        "\n",
        "    # Randomly rotate the image\n",
        "    angles = [0, 90, 180, 270]\n",
        "    angle = np.random.choice(angles)\n",
        "    image = scipy.ndimage.rotate(image, angle, axes=(0, 1), reshape=False, mode='nearest')\n",
        "\n",
        "    return image, label\n",
        "def augment_3d_and_display(image):\n",
        "    fig, axes = plt.subplots(1, 5, figsize=(20, 4))\n",
        "\n",
        "    # Original Image\n",
        "    axes[0].imshow(image[:, :, image.shape[2] // 2], cmap='gray')\n",
        "    axes[0].set_title('Original')\n",
        "\n",
        "    # Axial Flip\n",
        "    axial_flip = np.flip(image, axis=0)\n",
        "    axes[1].imshow(axial_flip[:, :, axial_flip.shape[2] // 2], cmap='gray')\n",
        "    axes[1].set_title('Axial Flip')\n",
        "\n",
        "    # Coronal Flip\n",
        "    coronal_flip = np.flip(image, axis=1)\n",
        "    axes[2].imshow(coronal_flip[:, :, coronal_flip.shape[2] // 2], cmap='gray')\n",
        "    axes[2].set_title('Coronal Flip')\n",
        "\n",
        "    # 90 Degree Rotation\n",
        "    rotation_90 = scipy.ndimage.rotate(image, 90, axes=(0, 1), reshape=False)\n",
        "    axes[3].imshow(rotation_90[:, :, rotation_90.shape[2] // 2], cmap='gray')\n",
        "    axes[3].set_title('90 Degree Rotation')\n",
        "\n",
        "    # 180 Degree Rotation\n",
        "    rotation_180 = scipy.ndimage.rotate(image, 180, axes=(0, 1), reshape=False)\n",
        "    axes[4].imshow(rotation_180[:, :, rotation_180.shape[2] // 2], cmap='gray')\n",
        "    axes[4].set_title('180 Degree Rotation')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def data_generator(X, y, batch_size=4, augment=False):\n",
        "\n",
        "    dataset_size = X.shape[0]\n",
        "    indices = np.arange(dataset_size)\n",
        "    np.random.shuffle(indices)\n",
        "\n",
        "    while True:\n",
        "        for start_idx in range(0, dataset_size, batch_size):\n",
        "            end_idx = min(start_idx + batch_size, dataset_size)\n",
        "            batch_indices = indices[start_idx:end_idx]\n",
        "\n",
        "            X_batch = np.empty((len(batch_indices),) + X.shape[1:], dtype=np.float32)\n",
        "            y_batch = y[batch_indices]\n",
        "\n",
        "            for i, idx in enumerate(batch_indices):\n",
        "                image, label = X[idx], y_batch[i]\n",
        "                if augment:\n",
        "                    image, label = augment_3d(image, label)\n",
        "                X_batch[i] = image\n",
        "\n",
        "            yield X_batch, y_batch\n",
        "\n",
        "assert tf.__version__.startswith('2.')\n",
        "\n",
        "\n",
        "\n",
        "def create_folder_if_not_exists(folder_path): #If a folder doesn't exist, create it\n",
        "    if not os.path.exists(folder_path):\n",
        "        os.makedirs(folder_path)\n",
        "        print(f\"Folder '{folder_path}' created.\")\n",
        "    else:\n",
        "        print(f\"Folder '{folder_path}' already exists.\")\n",
        "\n",
        "\n",
        "\n",
        "def load_nii(file_path): #load .nii images\n",
        "    img = nib.load(file_path).get_fdata()\n",
        "    img = np.expand_dims(img, axis=-1)\n",
        "    return img\n",
        "\n",
        "def load_data_and_labels(data_dir, labels_file): #load all the images and images\n",
        "    labels_df = pd.read_excel(labels_file)\n",
        "    labels_dict = dict(zip(labels_df.iloc[:, 0], labels_df.iloc[:, 1:].values))\n",
        "\n",
        "    data = []\n",
        "    labels = []\n",
        "    for filename in os.listdir(data_dir):\n",
        "        if filename.endswith('.nii'):\n",
        "            file_path = os.path.join(data_dir, filename)\n",
        "            img = load_nii(file_path)\n",
        "            key = filename.replace(\"smwp1\",\"smwp1_\").replace(\"_T1.nii\",\"\")\n",
        "            if key in labels_dict:\n",
        "                data.append(img)\n",
        "                labels.append(labels_dict[key])\n",
        "\n",
        "    print(\"Data Loaded\")\n",
        "    return np.array(data), np.array(labels)\n",
        "\n",
        "def normalize_data(input_data, output_data): #min-max normalization\n",
        "    print(np.min(input_data))\n",
        "    print(np.max(input_data))\n",
        "    print(np.min(output_data))\n",
        "    print(np.max(output_data))\n",
        "    input_norm = (input_data - np.min(input_data)) / (np.max(input_data) - np.min(input_data))\n",
        "    output_norm = (output_data - np.min(output_data)) / (np.max(output_data) - np.min(output_data))\n",
        "    print(\"Data Normalized\")\n",
        "    return input_norm, output_norm\n",
        "\n",
        "def preprocess_images(images, target_shape=(128, 144, 128)): #padded data for uniformity\n",
        "\n",
        "\n",
        "    pad_height = (target_shape[0] - images.shape[1]) / 2\n",
        "    pad_width = (target_shape[1] - images.shape[2]) / 2\n",
        "    pad_depth = (target_shape[2] - images.shape[3]) / 2\n",
        "\n",
        "    padding = (\n",
        "        (0, 0),\n",
        "        (int(np.floor(pad_height)), int(np.ceil(pad_height))),\n",
        "        (int(np.floor(pad_width)), int(np.ceil(pad_width))),\n",
        "        (int(np.floor(pad_depth)), int(np.ceil(pad_depth))),\n",
        "        (0, 0)\n",
        "    )\n",
        "\n",
        "    preprocessed_images = np.pad(images, padding, mode='constant', constant_values=0)\n",
        "\n",
        "    return preprocessed_images\n",
        "\n",
        "def resnet3d_block(input_tensor, filters, kernel_size=(3, 3, 3), strides=(1, 1, 1)):\n",
        "\n",
        "    x = layers.Conv3D(filters, kernel_size, padding='same', strides=strides)(input_tensor) #conv3d\n",
        "    x = layers.BatchNormalization()(x) #bn\n",
        "    x = layers.Activation('relu')(x) #relu\n",
        "\n",
        "    x = layers.Conv3D(filters, kernel_size, padding='same')(x) #conv3d\n",
        "    x = layers.BatchNormalization()(x) #bn\n",
        "\n",
        "    x = layers.add([x, input_tensor]) #add previous copy of input to current features through skip connection\n",
        "\n",
        "    return layers.Activation('relu')(x) #relu on the added input+current features again\n",
        "\n",
        "def build_3d_resnet(input_shape=(128, 128, 128, 1), num_classes=1):\n",
        "    inputs = layers.Input(shape=input_shape) #Input Layer Initialized\n",
        "\n",
        "    x = layers.Conv3D(64, (7, 7, 7), strides=(2, 2, 2), padding='same')(inputs) #Conv3D on the input with 64 filters, stride=2 and keeping the padding same so size of input and output are same\n",
        "    x = layers.BatchNormalization()(x) #Conv3d output batch normalized mean output close to 0 and the output standard deviation close to 1, regularized\n",
        "    x = layers.Activation('relu')(x) #non-linearity added between input and output with non-negative values\n",
        "    x = layers.MaxPooling3D((3, 3, 3), strides=(2, 2, 2), padding='same')(x) #downsamples x along with dim reduction along each channel with a stride of 2 taking max value over each 3*3*3 voxel and padding same to keep more info at edges\n",
        "\n",
        "\n",
        "    x = resnet3d_block(x, 64)\n",
        "    x = resnet3d_block(x, 64) #two blocks of resnet for simplication\n",
        "\n",
        "\n",
        "    x = layers.GlobalAveragePooling3D()(x)  #avg of feature map to a single scalar value - (32, 36, 32, 64) to (,64)\n",
        "    outputs = layers.Dense(num_classes, activation='linear')(x) #Dense Layer with 1 output (the predicted regression value) - (,64) to (,1)\n",
        "    model = models.Model(inputs, outputs)\n",
        "    return model\n",
        "\n",
        "\n",
        "def root_mean_squared_error(y_true, y_pred):\n",
        "    return tf.sqrt(tf.reduce_mean(tf.square(y_pred - y_true)))\n",
        "\n",
        "def mean_squared_error(y_true, y_pred):\n",
        "    return tf.reduce_mean(tf.square(y_pred - y_true))\n",
        "\n",
        "def load_and_preprocess_data(data_dir, labels_file):\n",
        "    data, labels = load_data_and_labels(data_dir, labels_file) #load data and labels into data,labels from the directory\n",
        "    data, labels = normalize_data(data, labels) #normalize the data\n",
        "    data = preprocess_images(data, target_shape=(128, 144, 128)) #reshape data by padding it and adding uniformity\n",
        "    print(\"Data Loaded and Preprocessed. Data shape:\", data.shape)\n",
        "\n",
        "    X_train, X_val, y_train, y_val = train_test_split(data, labels, test_size=0.20, random_state=42) #split data into train and val in 80:20 ratio\n",
        "\n",
        "    return X_train, X_val, y_train, y_val\n",
        "\n",
        "def main():\n",
        "    #Initialize all the folders\n",
        "    data_dir = '/content/drive/MyDrive/Data Mining 8650/n171_smwp1/'\n",
        "    labels_file = '/content/drive/MyDrive/Data Mining 8650/PTs_500_4k_blinded.xlsx'\n",
        "    tensorboard_log_dir = '/content/drive/MyDrive/Data Mining 8650/logs'\n",
        "\n",
        "    create_folder_if_not_exists(tensorboard_log_dir)\n",
        "    print(\"Directory Called\")\n",
        "\n",
        "\n",
        "    X_train, X_val, y_train, y_val = load_and_preprocess_data(data_dir, labels_file) #Preprocess the data\n",
        "\n",
        "    #first_image = X_train[0]\n",
        "    #augment_3d_and_display(first_image)\n",
        "\n",
        "    model = build_3d_resnet(input_shape=X_train.shape[1:], num_classes=1) #build the resnet model with height,width,depth,channels of the images with 1 output (predicted threshold)\n",
        "\n",
        "    model.summary() #show a summary of the layers, output shape, # of Parameters\n",
        "\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error', metrics=[mean_squared_error, root_mean_squared_error])\n",
        "\n",
        "    tensorboard_cb = TensorBoard(log_dir=tensorboard_log_dir)\n",
        "    model_checkpoint_cb = ModelCheckpoint(filepath='model_checkpoint.h5', save_best_only=True, monitor='val_loss', mode='min')\n",
        "\n",
        "\n",
        "    train_gen = data_generator(X_train, y_train, batch_size=4, augment=True)\n",
        "    val_gen = data_generator(X_val, y_val, batch_size=4)\n",
        "\n",
        "    train_steps_per_epoch = np.ceil(len(X_train) / 4).astype(int)\n",
        "    val_steps_per_epoch = np.ceil(len(X_val) / 4).astype(int)\n",
        "\n",
        "    model.fit(train_gen, steps_per_epoch=train_steps_per_epoch, validation_data=val_gen, validation_steps=val_steps_per_epoch, epochs=1000, callbacks=[tensorboard_cb, model_checkpoint_cb])\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GIMbf8OOor_p"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir \"/content/drive/MyDrive/Data Mining 8650/logs\"\n",
        "\n",
        "import IPython\n",
        "\n",
        "display(IPython.display.HTML('''\n",
        "<button id='open_tb'>Open TensorBoard</button>\n",
        "<button id='hide_tb'>Hide TensorBoard</button>\n",
        "<script>document.querySelector('#open_tb').onclick = () => { window.open(document.querySelector('iframe').src, \"__blank\") }\n",
        "        document.querySelector('#hide_tb').onclick = () => { document.querySelector('iframe').style.display = \"none\" }</script>'''))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}